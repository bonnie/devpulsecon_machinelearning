{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature pre-procession and hyper-parameter tuning lab\n",
    "\n",
    "In this lab we continue to use Ames housing dataset to practice the model optimization techniques.\n",
    "There are 3 places need to be completed:\n",
    "1. Scale features\n",
    "2. Regularize features\n",
    "3. Find the best hyper-parameter alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "import sys\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "We will use the Ames housing dataset from http://ww2.amstat.org/publications/jse/v19n3/decock.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('AmesHousing.txt', delimiter='\\t')\n",
    "numeric_values = np.where(\n",
    "    (data.dtypes == np.dtype('int64'))\n",
    "    | (data.dtypes == np.dtype('float64'))\n",
    ")[0]\n",
    "X = data[numeric_values[2:-1]].values\n",
    "y = data['SalePrice'].values\n",
    "feature_names = data.columns[numeric_values[2:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numeric_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.nan_to_num(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the data\n",
    "\n",
    "Let's take a quick look at the distribution of variables, looking for potential sources of outliers or features that may reduce the performance of our model.\n",
    "\n",
    "We'll use whisker plots to visualize all of the features at once. The organge line is the median value, the edges of the box denote the 1st and 3rd quartile, the bars at the end denote the maxium and minimum values, and individual points within the top and bottom quartile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "ax.boxplot(X_train)\n",
    "ax.set_xticks(range(1, X_train.shape[1] + 1))\n",
    "_ = ax.set_xticklabels(feature_names, rotation=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale features\n",
    "The scales of different features have a large variance. It could affect model performance. Rescale the range of each feature to [0, 1]. \n",
    "\n",
    "For each feature, use formula \n",
    "```\n",
    "x_scaled = (x - min) / (max - min)\n",
    "```\n",
    "where min and max are the minimum and maximum values of the feature in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect the data after scaling\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "ax.boxplot(X_train_scaled)\n",
    "ax.set_xticks(range(1, X_train_scaled.shape[1] + 1))\n",
    "_ = ax.set_xticklabels(feature_names, rotation=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Without scaling\n",
    "\n",
    "clf = LinearRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "fit_r2 = clf.score(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "mse_pred = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "y_min = np.min(y_test)\n",
    "y_max = np.max(y_test)\n",
    "ax.plot([y_min, y_max], [y_min, y_max], 'k--')\n",
    "ax.plot(y_test, y_pred, 'o', alpha=0.3)\n",
    "ax.set_xlabel('True Value')\n",
    "ax.set_ylabel('Predicted Value')\n",
    "\n",
    "print('Model Coefficients')\n",
    "for fn, coef in zip(feature_names, clf.coef_):\n",
    "    print('%10s: %.2f' % (fn, coef))\n",
    "print('\\nFit R^2 = %.2f, prediction MSE = %.5f' % (fit_r2, mse_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With scaling\n",
    "\n",
    "clf = LinearRegression()\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "fit_r2 = clf.score(X_train_scaled, y_train)\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "scaled_mse_pred = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "y_min = np.min(y_test)\n",
    "y_max = np.max(y_test)\n",
    "ax.plot([y_min, y_max], [y_min, y_max], 'k--')\n",
    "ax.plot(y_test, y_pred, 'o', alpha=0.3)\n",
    "ax.set_xlabel('True Value')\n",
    "ax.set_ylabel('Predicted Value')\n",
    "\n",
    "print('Model Coefficients')\n",
    "for fn, coef in zip(feature_names, clf.coef_):\n",
    "    print('%10s: %.2f' % (fn, coef))\n",
    "print('\\nFit R^2 = {0:.2f}, prediction MSE = {1:.5f}, prediction MSE reduction = {2:.5f}%'.format(\n",
    "      fit_r2, scaled_mse_pred, (mse_pred - scaled_mse_pred) / mse_pred * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularize features\n",
    "\n",
    "Use lasso with L1 norm to penalize non-zero coefficients. It can effectively reduce the number of variables on which the model is dependent.\n",
    "\n",
    "Objective function of lasso:\n",
    "```\n",
    "(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=90)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "lasso_fit_r2 = lasso.score(X_train_scaled, y_train)\n",
    "y_pred = lasso.predict(X_test_scaled)\n",
    "lasso_mse_pred = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "y_min = np.min(y_test)\n",
    "y_max = np.max(y_test)\n",
    "ax.plot([y_min, y_max], [y_min, y_max], 'k--')\n",
    "ax.plot(y_test, y_pred, 'o', alpha=0.3)\n",
    "ax.set_xlabel('True Value')\n",
    "ax.set_ylabel('Predicted Value')\n",
    "\n",
    "print('Model Coefficients')\n",
    "for fn, coef in zip(feature_names, lasso.coef_):\n",
    "    print('%10s: %.2f' % (fn, coef))\n",
    "print('\\nFit R^2 = {0:.2f}, prediction MSE = {1:.5f}, prediction MSE reduction = {2:.5f}%'.format(\n",
    "      lasso_fit_r2, lasso_mse_pred, (mse_pred - lasso_mse_pred) / mse_pred * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best alpha\n",
    "\n",
    "Use grid search to find the best alpha in lasso that gives the minimum test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphas = [i for i in range(0, 200, 10)]\n",
    "mse_pred_array = []\n",
    "best_mse = sys.maxint\n",
    "best_alpha = 0\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    y_pred = lasso.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_pred_array.append(mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_alpha = alpha\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "y_min = np.min(mse_pred_array)\n",
    "y_max = np.max(mse_pred_array)\n",
    "ax.plot(alphas, mse_pred_array, '*-')\n",
    "ax.set_xlabel('Alpha')\n",
    "ax.set_ylabel('MSE')\n",
    "\n",
    "print('\\nBest alpha = {0}, best prediction MSE = {1:.5f}, reduction = {2:.4f}%'.format(\n",
    "    best_alpha, best_mse, (mse_pred - best_mse) / mse_pred * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bonus Point\n",
    "\n",
    "* Try other normalization methods and compare the difference. \n",
    "* Convert the non-numeric features to binary features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
