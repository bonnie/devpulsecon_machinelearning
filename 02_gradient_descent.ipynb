{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Mini Lab\n",
    "\n",
    "In this short lab we will visualize the gradient descent algorithm and build a better understanding of the algorithm's hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import scipy.stats\n",
    "import scipy.misc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the \"loss\" \n",
    "\n",
    "Ideally, the error has a nice convex shape, like a quadratic function. For the purposes of this exercise we'll use a two-dimensional normal distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cx = 0.5\n",
    "cy = -0.2\n",
    "\n",
    "lim = 1.5\n",
    "x, y = np.mgrid[-lim:lim:.01, -lim:lim:.01]\n",
    "pos = np.empty(x.shape + (2,))\n",
    "pos[:, :, 0] = x; pos[:, :, 1] = y\n",
    "rv = scipy.stats.multivariate_normal([cx, cy], [[2.0, 0.3], [0.5, 0.5]])\n",
    "pdf = lambda x: -rv.pdf(x)\n",
    "z = pdf(pos)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_wireframe(x, y, z, rstride=30, cstride=30, alpha=0.3)\n",
    "ax.plot_surface(x, y, z, rstride=5, cstride=5, cmap=cm.rainbow,\n",
    "                linewidth=0, antialiased=False, alpha=0.3)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Gradient Descent\n",
    "\n",
    "Let's learn the minima of the loss function. We'll do this by implementing a toy gradient descent algorithm and plotting the sequence of positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model hyper-parameters\n",
    "lr = 1  # The learning rate for each update\n",
    "n_iter = 1000  # Max number of itteration before the algorithm should give up\n",
    "eps = 0.0001  # The threshold at which we will consider the algorithm as having converged\n",
    "\n",
    "# Fixed initial guess for values\n",
    "# posx = [1.08]\n",
    "# posy = [0.57]\n",
    "\n",
    "# Random initial guess for values\n",
    "posx = [np.random.random()*2*lim - lim]\n",
    "posy = [np.random.random()*2*lim - lim]\n",
    "\n",
    "def partial_derivative(func, var=0, point=[]):\n",
    "    args = point[:]\n",
    "    def dim_func(x):\n",
    "        args[var] = x\n",
    "        return func(args)\n",
    "    return scipy.misc.derivative(dim_func, point[var], dx = 1e-6)\n",
    "\n",
    "# Walk along the gradient till we find a solution\n",
    "for i in range(n_iter):\n",
    "    x0 = posx[-1]\n",
    "    y0 = posy[-1]\n",
    "    posx.append(x0 - lr * partial_derivative(pdf, 0, [x0, y0]))\n",
    "    posy.append(y0 - lr * partial_derivative(pdf, 1, [x0, y0]))\n",
    "    if np.abs(np.diff(pdf(zip(posx[-2:], posy[-2:])))) < eps:\n",
    "        break\n",
    "\n",
    "posx = np.array(posx)\n",
    "posy = np.array(posy)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16,4))\n",
    "\n",
    "ax = axs[0]\n",
    "ax.contour(x, y, z, cmap=cm.rainbow)#cmap=cm.cool,)\n",
    "ax.plot(cx, cy, 'x')\n",
    "ax.plot(posx[0], posy[0], 's')\n",
    "ax.quiver(\n",
    "    posx[:-1], posy[:-1], \n",
    "    np.diff(posx), np.diff(posy), \n",
    "    scale_units='xy', \n",
    "    angles='xy', \n",
    "    scale=1., \n",
    "    headwidth=5,\n",
    ")\n",
    "ax.set_xlim(-lim, lim)\n",
    "ax.set_ylim(-lim, lim)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(posx - cx, label='X')\n",
    "ax.plot(posy - cy, label='Y')\n",
    "ax.set_xlabel('Itteration')\n",
    "ax.set_ylabel('Distance To Minima')\n",
    "ax.legend(loc='best')\n",
    "\n",
    "ax = axs[2]\n",
    "ax.plot(pdf(zip(posx, posy)))\n",
    "ax.plot([0, i], [pdf((cx, cy))]*2, 'k--')\n",
    "ax.set_xlabel('Itteration')\n",
    "ax.set_ylabel('Loss')\n",
    "\n",
    "print 'Terminated after %d of %d itterations' % (i, n_iter)\n",
    "print 'Initial Guess at (%.2f, %.2f)' % (posx[0], posy[0])\n",
    "print 'Fit Minima at (%.2f, %.2f)' % (posx[-1], posy[-1])\n",
    "print 'True Minima at (%.2f, %.2f)' % (cx, cy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Questions\n",
    "\n",
    "* How are the number of iterations and final solution affected by:\n",
    " * The initial guess\n",
    " * The size of the learning rate\n",
    " * The epsilon\n",
    " * The magnitude of the gradient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
