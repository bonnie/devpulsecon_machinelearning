{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Main Lab\n",
    "\n",
    "The aim of this lab is to apply the methods and tricks you have learned to improve the quality of the fit. A bare-bones fit using a linear regression on the input features has been included as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "We will use the Ames housing dataset from http://ww2.amstat.org/publications/jse/v19n3/decock.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('AmesHousing.txt', delimiter='\\t')\n",
    "numeric_values = np.where(\n",
    "    (data.dtypes == np.dtype('int64'))\n",
    "    | (data.dtypes == np.dtype('float64'))\n",
    ")[0]\n",
    "X = data[numeric_values[2:-1]].values\n",
    "y = data['SalePrice'].values\n",
    "feature_names = data.columns[numeric_values[2:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numeric_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.nan_to_num(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the data\n",
    "\n",
    "Let's take a quick look at the distribution of variables, looking for potential sources of outliers or features that may reduce the performance of our model.\n",
    "\n",
    "We'll use whisker plots to visualize all of the features at once. The organge line is the median value, the edges of the box denote the 1st and 3rd quartile, the bars at the end denote the maxium and minimum values, and individual points within the top and bottom quartile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.boxplot(X_train)\n",
    "ax.set_xticks(range(1, X_train.shape[1] + 1))\n",
    "_ = ax.set_xticklabels(feature_names, rotation=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LinearRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "fit_r2 = clf.score(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "mse_pred = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "y_min = np.min(y_test)\n",
    "y_max = np.max(y_test)\n",
    "ax.plot([y_min, y_max], [y_min, y_max], 'k--')\n",
    "ax.plot(y_test, y_pred, 'o', alpha=0.3)\n",
    "ax.set_xlabel('True Value')\n",
    "ax.set_ylabel('Predicted Value')\n",
    "\n",
    "print('Model Coefficients')\n",
    "for fn, coef in zip(feature_names, clf.coef_):\n",
    "    print('%10s: %.2f' % (fn, coef))\n",
    "print('\\nFit R^2 = %.2f, prediction MSE = %.5f' % (fit_r2, mse_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Questions\n",
    "\n",
    "* Which features have the most predictive power?\n",
    "* Should any of the features be pre-processed?\n",
    "  * How could we include the non-numeric features?\n",
    "* Can we improve the model by including higher-order terms?\n",
    "* Do we risk over-fitting this data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
