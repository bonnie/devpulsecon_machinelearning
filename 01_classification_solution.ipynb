{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports for Reading data\n",
    "import pandas as pd\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv(\"ClassificationData.csv\", thousands=',', sep=',',engine='python')\n",
    "# Get the class labels for each datapoint in a separate array\n",
    "df_data_y = df_data['Label']\n",
    "df_data_x = df_data.drop('Label', 1)\n",
    "\n",
    "# no of rows and columns in the data set\n",
    "df_data_x.shape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Logistic regression\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "# Support Vector Machines\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Decision Trees\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset into test and train\n",
    "# x is data, y is labels\n",
    "# bds: do this many times (cross-validation) to avoid \"over fitting\" data. \n",
    "X_train, X_test, y_train, y_test = train_test_split(df_data_x, df_data_y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.09135721e-09  -1.87483718e-06  -5.27858422e-06  -1.00362136e-05\n",
      "    1.31749744e-04  -3.42659510e-08   6.25826599e-09   5.29333710e-08\n",
      "    2.85780048e-08  -6.43138443e-08  -2.77591419e-08   8.46981362e-08\n",
      "   -5.27429960e-07   6.18223338e-07   2.41921647e-05  -3.32989057e-09\n",
      "   -2.23289106e-09  -1.64867602e-09  -9.99736171e-10  -8.94891815e-09\n",
      "   -1.26258488e-09  -9.16556329e-07  -6.51245076e-06  -3.42555918e-06\n",
      "    3.06964479e-04  -4.35826298e-08   3.99053431e-08   1.04220233e-07\n",
      "    3.48018827e-08  -8.70377567e-08  -2.75642659e-08]]\n",
      "0.395604395604\n",
      "0.298245614035\n"
     ]
    }
   ],
   "source": [
    "# Classification using Logistic Regression \n",
    "\n",
    "# step size in the mesh\n",
    "h = .02  \n",
    "\n",
    "# Fit the data to teh model to get the weights for features\n",
    "# logistic regression is the classifier\n",
    "# C is the inverse of some metric. The lower C, the stricter the regularization (penalize model more for wrong predictions)\n",
    "logreg = linear_model.LogisticRegression(C=1.0)\n",
    "lg = logreg.fit(X_train, y_train)\n",
    "\n",
    "# coefficient associated with each feature\n",
    "print(lg.coef_)\n",
    "\n",
    "# Use the Log reg model to calculate predict the class labels\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_proba = logreg.predict_proba(X_test)\n",
    "\n",
    "# Logistic Regression score (accuracy) on training and test\n",
    "# score gives accuracy\n",
    "# bds: this led to pretty poor accuracy for our random training set\n",
    "print(logreg.score(X_train, y_train))\n",
    "print(logreg.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machine Model\n",
    "\n",
    "#split the dataset into test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_data_x, df_data_y, test_size = 0.2)\n",
    "\n",
    "# Fit the training data\n",
    "# bds: kernel is the boundary shape can be linear (boundary is linear). rbf stands for radial basis function\n",
    "svm = SVC(C=0.1, kernel='rbf')\n",
    "svm.fit(X_train, y_train)\n",
    "print \"I'm done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.615384615385\n",
      "0.675438596491\n"
     ]
    }
   ],
   "source": [
    "# Check predictions on training data\n",
    "y_pred_svm = svm.predict(X_train)\n",
    "\n",
    "# The mean accuracy on test and training \n",
    "print(svm.score(X_train, y_train))\n",
    "print(svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification using Decision Trees\n",
    "\n",
    "dt_model = DecisionTreeClassifier(min_samples_split=20)\n",
    "dt_model.fit(X_train, y_train)\n",
    "print(dt_model)\n",
    "\n",
    "# make predictions\n",
    "y_pred_decision_tree = dt_model.predict(X_test)\n",
    "\n",
    "# Accuracy of decision tree model on training data\n",
    "dt_model.score(X_train, y_train) \n",
    "\n",
    "# Accuracy of decision tree model on test dataset\n",
    "dt_model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
